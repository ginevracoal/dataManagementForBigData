---
title: "Scalable Data Processing in R"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, eval = FALSE)
```

Datasets are often larger than available RAM, which causes problems for R programmers since by default all the variables are stored in memory. You’ll learn tools for processing, exploring, and analyzing data directly from disk. You’ll also implement the split-apply-combine approach and learn how to write scalable code using the bigmemory and iotools packages. You'll make use of the Federal Housing Finance Agency's data, a publicly available data set chronicling all mortgages that were held or securitized by both Federal National Mortgage Association (Fannie Mae) and Federal Home Loan Mortgage Corporation (Freddie Mac) from 2009-2015.

This page is based on [Scalable Data Processing in R](https://www.datacamp.com/courses/scalable-data-processing-in-r) DataCamp course by Michael Kane and Simon Urbanek.

Datasets:

* [mortgage-sample.csv](mortgage-sample.csv)

## Working with bigmemory

```{r}
# Load the bigmemory package
library(bigmemory)

# Create the big.matrix object. The result will be a file on the disk holding the value read in (backing file) along with a descriptor file which holds extra information, like the number of columns and rows (descriptor file)
x <- read.big.matrix("mortgage-sample.csv", header = TRUE, 
                     type = "integer", 
                     backingfile = "mortgage-sample.bin", 
                     descriptorfile = "mortgage-sample.desc")
                    
# creates a reference to a previously allocated file-backed matrices. 
mort <- attach.big.matrix("mortgage-sample.desc")

# Names of columns
col_names <- colnames(mort)
col_names

# Find the dimensions of mort
dim(mort)

# Look at the first 3 rows
mort[1:3, ]

# Create a table of the number of mortgages for each year in the data set
table(mort[, "year"])

# Load the biganalytics package
library(biganalytics)

# Get the column means of mort
colmean(mort)

# Get a summary of the data
summary(mort)


# Load the bigtabulate package
library(bigtabulate)

# Call bigtable to create a variable called race_table
race_table <- bigtable(mort, "borrower_race")
race_cat = c("Native Am", "Asian", "Black", "Pacific Is",  "White", "Two or More", "Hispanic", "Not Avail")
names(race_table) <- race_cat
race_table

# Create a table of the borrower race by year
race_year_table <- bigtable(mort, c("borrower_race", "year"))

# Convert matrix to a data frame
rydf <- as.data.frame(race_year_table)

# Create the new column Race
rydf$Race <- race_cat

# Let's see what it looks like
rydf

```

### split-apply-combine strategy on big matrices

```{r}

# compute proporation of female borrowers in urban and ruran areas
female_residence_prop <- function(x, rows) {
    # find subset
    x_subset <- x[rows, ]
    # Find the proporation of female borrowers in urban areas
    prop_female_urban <- sum(x_subset[, "borrower_gender"] == 2 & x_subset[, "msa"] == 1) / sum(x_subset[, "msa"] == 1)
    # Find the proporation of female borrowers in rural areas
    prop_female_rural <- sum(x_subset[, "borrower_gender"] == 2 & x_subset[, "msa"] == 0) / sum(x_subset[, "msa"] == 0)
    c(prop_female_urban, prop_female_rural)
}

# Find the proportion of female borrowers in 2015
female_residence_prop(mort, mort[, "year"] == 2015)

# Split: divide the row numbers of the mortage data by year
spl <- split(1:nrow(mort), mort[, "year"])
str(spl)

# Apply: for each of the row splits, find the female residence proportion
all_years <- Map(function(rows) female_residence_prop(mort, rows), spl)
str(all_years)

# Combine: collect the results as rows in a matrix
prop_female <- Reduce(rbind, all_years)

# Rename the row and column names
dimnames(prop_female) <- list(names(all_years), c("prop_female_urban", "prop_female_rural"))

# View the matrix
prop_female

# Load the dplyr and ggplot2 packages
library(dplyr)
library(ggplot2)

# Convert prop_female to a data frame (tibble)
prop_female_df <- as_tibble(prop_female)

# Add a new column Year
prop_female_df <- mutate(prop_female_df, Year = row.names(prop_female))

# gather prop_female_df and prop_female_urban columns
prop_female_long <- gather(prop_female_df, prop_female_urban, prop_female_rural, key = "Region", value = "Prop")
prop_female_long

# Create a line plot
ggplot(prop_female_long, aes(x = Year, y = Prop, group = Region, color = Region)) +
    geom_line()

```

## Working with iotools

```{r}

# Load the iotools and microbenchmark packages
library(iotools)
library(microbenchmark)

# raw (byte) vectors
as.raw(0:255)

# reading raw data is more efficient
microbenchmark(
    # Time the reading of a file using read.delim five times
    read.delim("mortgage-sample.csv", header = FALSE, sep = ","),
    # pre-loads the data as a raw vector and parses without constructing intermediate strings
    read.delim.raw("mortgage-sample.csv", header = FALSE, sep = ","),
    times = 5
)

# Read mortgage-sample.csv as a raw vector
raw_file_content <- readAsRaw("mortgage-sample.csv")

# Convert the raw vector contents to a matrix
mort_mat <- mstrsplit(raw_file_content, sep = ",", type = "integer", skip = 1)

# Look at the first 6 rows
head(mort_mat)

# Convert the raw file contents to a data.frame
mort_df <- dstrsplit(raw_file_content, sep = ",", col_types = rep("integer", 16), skip = 1)

# Look at the first 6 rows
head(mort_df)

```


### split-apply-combine strategy on iotools

```{r}

# Define the function to apply to each chunk
make_table <- function(chunk) {
    # Read each chunk as a matrix
    x <- mstrsplit(chunk, type = "integer", sep = ",")
    # Create a table of the number of borrowers (column 3) for each chunk
    table(x[, 3])
}


# Create a file connection to mortgage-sample.csv (oper in reading in binary mode)
fc <- file("mortgage-sample.csv", "rb")

# Read the first line to get rid of the header
readLines(fc, n = 1)

# Read the data in chunks
counts <- chunk.apply(fc, make_table, CH.MAX.SIZE = 1e5)

# Close the file connection
close(fc)

# Print counts
counts

# Sum up the chunks
colSums(counts)


# Let's work with data frames
msa_map <- c("rural", "urban")

# Define the function to apply to each chunk
make_msa_table <- function(chunk) {
    # Read each chunk as a data frame
    x <- dstrsplit(chunk, col_types = rep("integer", length(col_names)), sep = ",")
    # Set the column names of the data frame that's been read
    colnames(x) <- col_names
    # Create new column, msa_pretty, with a string description of where the borrower lives
    x$msa_pretty <- msa_map[x$msa + 1]
    # Create a table from the msa_pretty column
    table(x$msa_pretty)
}

# Create a file connection to mortgage-sample.csv
fc <- file("mortgage-sample.csv", "rb")

# Read the first line to get rid of the header
readLines(fc, n = 1)

# Read the data in chunks
counts <- chunk.apply(fc, make_msa_table, CH.MAX.SIZE = 1e5)

# Close the file connection
close(fc)

# Aggregate the counts as before
colSums(counts)


# Let's work in parallel
iotools_read_fun <- function(parallel) {
    fc <- file("mortgage-sample.csv", "rb")
    readLines(fc, n = 1)
    chunk.apply(fc, make_msa_table,
                CH.MAX.SIZE = 1e5, parallel = parallel)
    close(fc)
}

# Benchmark the new function
microbenchmark(
    # Use one process
    iotools_read_fun(parallel = 1), 
    # Use three processes
    iotools_read_fun(parallel = 3), 
    times = 20
)

```



